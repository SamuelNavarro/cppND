\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{minted}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}
\graphicspath{{./images/}}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\begin{document}
\title{Concurrency}
\author{Samuel Navarro}
\date{\today}
\maketitle
\tableofcontents{}

\section{Introduction}%
\label{sec:introduction}

Concurrency means that a single system is performing multiple, independent tasks in parallel rather than sequentially as before. Such a system is called a multi-tasking system. 

Multi-tasking does not require multiple processes or multiple cores. Dividing a program execution into many small bits and pieces and by interleaving them with a bits of pieces of other programs we create the illusion of parallelism, which is maintained by the OS. Under the hood we have an extremely fast switching of tasks. 

But true concurrency with tasks that are running at the same time require a parallel architecture. This might mean having multiple processors or multiple cores, or maybe both multiple cores within a processor. This is called \textbf{hardware concurrency.}


An important measure which helps to gauge how many independent tasks can be run in parallel, this is the number of hardware threads on a system. 


\section{Introduction and Running Threads}%
\label{sec:introduction_and_running_threads}



\subsection{Processes and Threads}%
\label{sec:processes_and_threads}



A parallel path of execution is one which runs concurrently with the main program and is thus asynchronous. In contrast to synchronous programs, the main program can continue with its line of execution without the need to wait for the parallel task to complete. The following figure illustrates this difference.




\begin{figure}[htpb!]
	\centering
	\includegraphics[width=0.8\linewidth]{async_sync}
	\caption{Asynchronous vs Synchronous}
	\label{fig:async_sync}
\end{figure}


A \textbf{process} (also called a \textbf{task}) is a computer program at runtime. It is comprised of the runtime environment provided by the OS, as well as of the embedded binary code of the program during execution. A \textbf{process} is controlled by the OS through certain actions with which it sets the process into one of several carefully defined states:



\begin{figure}[htpb!]
	\centering
	\includegraphics[width=0.8\linewidth]{process}
	\caption{Process}
	\label{fig:process}
\end{figure}


\begin{itemize}
	\item \textbf{Ready} : After its creation, a process enters the ready state and is loaded into main memory. The process now is ready to run and is waiting for CPU time to be executed. Processes that are ready for execution by the CPU are stored in a queue managed by the OS.
	\item \textbf{Running} : The operating system has selected the process for execution and the instructions within the process are executed on one or more of the available CPU cores.
	\item \textbf{Blocked} : A process that is blocked is one that is waiting for an event (such as a system resource becoming available) or the completion of an I/O operation.
	\item \textbf{Terminated} : When a process completes its execution or when it is being explicitly killed, it changes to the "terminated" state. The underlying program is no longer executing, but the process remains in the process table as a "zombie process". When it is finally removed from the process table, its lifetime ends.
	\item \textbf{Ready suspended}: A process that was initially in ready state but has been swapped out of main memory and placed onto external storage is said to be in suspend ready state. The process will transition back to ready state whenever it is moved to main memory again.
	\item \textbf{Blocked suspended} : A process that is blocked may also be swapped out of main memory. It may be swapped back in again under the same conditions as a "ready suspended" process. In such a case, the process will move to the blocked state, and may still be waiting for a resource to become available.
\end{itemize}





Processes are managed by the scheduler of the OS. The scheduler can either let a process run until it ends or blocks (non-interrupting scheduler), or it can ensure that the currently running process is interrupted after a short period of time. The scheduler can switch back and forth between different active processes (interrupting scheduler), alternately assigning them CPU time. The latter is the typical scheduling strategy of any modern operating system.

Since the administration of processes is computationally taxing, operating systems support a more resource-friendly way of realizing concurrent operations: the threads.


A \textit{thread} represents a concurrent execution unit within a process. In contrast to full-blown processes as described above, threads are characterized as light-weight processes (LWP). There are significantly easier to create and destroy: In many systems the creation of a thread is up to 100 times faster than the creation of a process. This is especially advantageous in situations, when the need for concurrent operations changes dynamically. 


\begin{figure}[htpb!]
	\centering
	\includegraphics[width=0.8\linewidth]{process_threads}
	\caption{Process and Threads}
	\label{fig:process_threads}
\end{figure}




Threads exist within processes and share their resources. As illustrated by the Figure~\ref{fig:process_threads}, a process can contain several threads or - if no parallel processing is provided for in the program flow - only a single thread.

A major difference between a process and a thread is that each process has its own address space, while a thread does not require a new address space to be created. All the threads in a process can access its shared memory. Threads also share other OS dependent resources such as processors, files, and network connections. As a result, the management overhead for threads is typically less than for processes. Threads, however, are not protected against each other and must carefully synchronize when accessing the shared process resources to avoid conflicts.


Similar to processes, threads exist in different states, which are illustrated in the Figure~\ref{fig:threads}: 


\begin{figure}[htpb!]
	\centering
	\includegraphics[width=0.8\linewidth]{threads}
	\caption{Threads}
	\label{fig:threads}
\end{figure}


\begin{itemize}
	\item \textbf{New} : A thread is in this state once it has been created. Until it is actually running, it will not take any CPU resources.
	\item \textbf{Runnable} : In this state, a thread might actually be running or it might be ready to run at any instant of time. It is the responsibility of the thread scheduler to assign CPU time to the thread.
	\item \textbf{Blocked} : A thread might be in this state, when it is waiting for I/O operations to complete. When blocked, a thread cannot continue its execution any further until it is moved to the runnable state again. It will not consume any CPU time in this state. The thread scheduler is responsible for reactivating the thread.
\end{itemize}




\begin{itemize}
	\item In a concurrent program, \textbf{threads} share memory. Thus, many \textbf{threads} can access and modify the same memory.
	\item Creating a \textbf{process} is fairly resource-intensive. It is generally more efficient to use several \textbf{threads} within a \textbf{process}.
	\item In contrast to a \textbf{process}, \textbf{threads} are characterized as light-weight. They are significantly easier to create and destroy.
	\item Inter-thread communication can be faster than inter-process communication.
\end{itemize}


\subsection{Starting a Second Thread}%
\label{sec:starting_a_second_thread}



If we want to start a second thread in addition to the main thread of our program, we need to construct a thread object and pass it the function we want to be executed by the thread. Once the thread enters the runnable state, the execution of the associated thread function may start at any point in time.

\texttt{std::thread t (threadFunction);} 


After the thread object has been constructed, the main thread will continue and execute the remaining instructions until it reaches the end and returns. It is possible that by this point in time, the thread will also have finished. But if this is not the case, the main program will terminate and the resources of the associated process will be freed by the OS. As the thread exists within the process, it can no longer access those resources and thus not finish its execution as intended.

To prevent this from happening and have the main program wait for the thread to finish the execution of the thread function, we need to call \texttt{join()} on the thread object. This call will only return when the thread reaches the end of the thread function and block the main thread until then.  

An example of this is in the file \texttt{main\_and\_sec\_threads.cpp}


\subsubsection{Randomness of events}%
\label{sub:randomness_of_events}

One very important trait of concurrent programs is their non-deterministic behavior. It can not be predicted which thread the scheduler will execute at which point in time. In the code on \texttt{randomness\_of\_events.cpp}, the amount of work to be performed both in the thread function and in main has been split into two separate jobs. The output in the console differs. 


\textbf{We can use \texttt{join()} as a barrier}

Because the order of execution is determined by the scheduler, if we wanted to ensure that the thread function completed its work before the main function started its own work \textbf{(because it might be waiting for a result to be available)}, we could achieve this by repositioning the call to join.


In the file, the \texttt{.join()} can be moved to before the work in \texttt{main()}. The order of execution now always looks the same.


\subsubsection{Detach}%
\label{sub:detach}



Let us now take a look at what happens if we don’t join a thread before its destructor is called. When we comment out join in the example above and then run the program again, it aborts with an error. The reason why this is done is that the designers of the C++ standard wanted to make debugging a multi-threaded program easier: Having the program crash forces the programmer to remember joining the threads that are created in a proper way. Such a hard error is usually much easier to detect than soft errors that do not show themselves so obviously.

There are some situations however, where it might make sense to not wait for a thread to finish its work. This can be achieved by "detaching" the thread, by which the internal state variable "joinable" is set to "false". This works by calling the \texttt{detach()} method on the thread. \textbf{The destructor of a detached thread does nothing: It neither blocks nor does it terminate the thread}. In the following example, detach is called on the thread object, which causes the main thread to immediately continue until it reaches the end of the program code and returns. Note that a detached thread can not be joined ever again.


Programmers should be very careful though when using the \texttt{detach()} method. You have to make sure that the thread does not access any data that might get out of scope or be deleted. Also, we do not want our program to terminate with threads still running. Should this happen, such threads will be terminated very harshly without giving them the chance to properly clean up their resources - what would usually happen in the destructor. So a well-designed program usually has a well-designed mechanism for joining all threads before exiting.





\subsection{Starting a Thread with a Function Object}%
\label{sec:starting_a_thread_with_a_function_object}

We now want to pass a function object instance to the thread. This is basically an instance of a class that implements the function call operator. This can be very useful to passing data through threads. Another way to pass data and also start a thread with a very small code footprint is to use a so-called Lambda function. 



Passing functions to other functions is one form of a \textit{callable object}.

In C++, callable objects are objects that can appear as the left-hand operand of the call operator. These can be pointers to functions, objects of a class that defines an overloaded function call operator and \textit{lambdas}, with which function objects can be created in a very simple way. In the context of \textbf{concurrency}, we can use callable objects to attach a function to a thread. 

In the Section~\ref{sec:starting_a_second_thread} we constructed a thread object by passing a function to it without any arguments. If we were limited to this approach, the only way to make data available from within the thread function would be to use global variables - \textit{which is definitely not recommendable and also incredibly messy.}





The extra parenthesis in the file \texttt{mostVexingParse} is known as C++'s \textit{most vexing parse}, which is a specific form of syntactic ambiguity resolution in the C++ programming language.

The \textit{"most vexing parse"} comes from a rule in C++ that says that anything that could be considered as a function declaration, the compiler should parse it as a function declaration - even if it could be interpreted as something else.


The line:

\texttt{std::thread t(Vehicle());} 

is seemingly ambiguous, since it could be interpreted either as:

\begin{enumerate}
	\item a variable definition for variable t of class \texttt{std::thread}, initialized with an anonymous instance of class Vehicle, or
	\item a function declaration for a function t that return an object of type \texttt{std::thread}  and has a single (unnamed) parameter that is a pointer to function returning an object of type Vehicle.
\end{enumerate}


Most programmers would presumable expect the first case to be true, but the C++ standard requires it to be interpreted as the second - hence the compiler warning. 


There are three ways of forcing the compiler to consider the line as the first case, which would create the thread object we want:


\begin{itemize}
  \item Add an extra pair of parentheses: \texttt{std::thread t1 ( (Vehicle()) );} 
	\item Use copy initialization. \texttt{std::thread t2 = std::thread( Vehicle() ) ;} 
	\item Use uniform initialization with braces. \texttt{std::thread t3\{Vehicle()\};} 
\end{itemize}

Whichever option we use, the idea is the same: the function object is copied into internal storage accessible to the new thread, and the new thread invokes the operator \texttt{()}. Th \texttt{Vehicle()} class can of course have data members and other member functions too, and this is one way of passing data to the thread function: pass it in as a constructor argument and store it as a data member.

File: \texttt{PassingDataToThread.cpp} 




\subsubsection{Lambdas}%
\label{sub:lambdas}


Another useful way of starting a thread and passing information to it is by using a lambda expression. C++ Lambdas have the properties of being unnamed and capturing variables from the surrounding context, but lack the ability to execute and return functions. 

A Lambda is often used as an argument for functions that can take a callable object. A Lambda is a function object, so it has a type and can be stored and passed around. Its result object is called a \textit{"closure"}, which can be called using the operator \texttt{()} 

It formally consists of three parts: a capture list \texttt{[]}, a parameter list \texttt{()} and a main part \texttt{\{\}}, which contains the code to be executed when the lambda is called. Note that in principal all parts could be empty.

The capture list \texttt{[]}: By default, variables outside of the enclosing {} around the main part of the Lambda can not be accessed. By adding a variable to the capture list however, it becomes available within the Lambda \textbf{either as a copy or as a reference}. The captured variables become a part of the Lambda.

By default, variables in the capture block can not be modified within the Lambda. Using the keyword "mutable" allows to modify the parameters captured by copy, and to call their non-const member functions within the body of the Lambda.


Even though we have been using Lambdas in the above example in various ways, it is important to note that a Lambda does not exist at runtime. The runtime effect of a Lambda is the generation of an object, which is known as closure. The difference between a Lambda and the corresponding closure is similar to the distinction between a class and an instance of the class. A class exists only in the source code while the objects created from it exist at runtime.

We can use (a copy of) the closure (i.e. f0, f1, …) to execute the code within the Lambda at a position in our program different to the line where the function object was created.

The parameter list () : The way parameters are passed to a Lambda is basically identical to calling a regular function. If the Lambda takes no arguments, these parentheses can be omitted (except when "mutable" is used).



\textbf{Question} What's the difference between capture list and parameter list. See this \href{https://stackoverflow.com/questions/28669941/c-lambdas-capture-list-vs-parameter-list}{link}

It seems that the capture list is to keep a state.


\begin{listing}
\begin{minted}[linenos,numbersep=5pt,frame=lines,framesep=2mm]{cpp}
#include <iostream>

int main(){
	const auto addSome = [](double some){
		return [some](double val){ return some + val;};
	};


	const auto addFive = addSome(5);

	std::cout << addFive(2) << std::endl;
	std::cout << addFive(3) << std::endl;
	/*
  OUTPUT:
  7
	8
	* */
	return 0;
}
\end{minted}
\caption{Capture vs Parameter List in Lambda Functions}
\label{lst:capture_vs_parameter_list_in_lambda_functions}
\end{listing}









Let's say we have this code snippet:




\begin{listing}
\begin{minted}[linenos,numbersep=5pt,frame=lines,framesep=2mm]{cpp}
#include <iostream>

int main()
{
    int id = 0; // Define an integer variable

    // capture by reference (immutable)
    auto f0 = [&id]() { std::cout << "a) ID in Lambda = " << id << std::endl; };

    // capture by value (mutable)
    auto f1 = [id]() mutable { std::cout << "b) ID in Lambda = " << ++id << std::endl; };
    f1(); // call the closure and execute the code witin the Lambda
    std::cout << "c) ID in Main = " << id << std::endl;

    // capture by reference (mutable)
    auto f2 = [&id]() mutable { std::cout << "d) ID in Lambda = " << ++id << std::endl; };
    f2(); 
    std::cout << "e) ID in Main = " << id << std::endl; 

    // pass parameter 
    auto f3 = [](const int id) { std::cout << "f) ID in Lambda = " << id << std::endl; };   
		//Before we pass id into it, we increase it by one.
		//That's why we have id = 2
    f3(++id);

    // observe the effect of capturing by reference at an earlier point in time
		// Because we pass by reference the id, we have Id=2
    f0(); 

    return 0;
}
\end{minted}
\caption{Capture Variables by Value and Reference}
\label{lst:capture_variables_by_value_and_reference}
\end{listing}


Pass by value means that we are going to make a copy of the parameter; we're not going to change the original ID. But, because the lambda is \texttt{mutable} that means that ID can be change within the Lambda itself. 


In Listing~\ref{lst:capture_variables_by_value_and_reference}. The output will be:
\begin{itemize}
	\item b) ID lambda = 1
	\item c) ID Main = 0
	\item d) ID lambda = 1
	\item e) ID Main = 1
	\item f) ID lambda = 2
	\item a) Id lambda = 2
\end{itemize}





Another example is in file: \texttt{startThreadsLambdas.cpp}. Here, the output in the main thread is generated first, at which point the variable ID has taken the value 1. Then, the call-by-value thread is executed with ID at a value of 0. Then, the call-by-reference thread is executed with ID at a value of 1. This illustrates the effect of passing a value by reference : when the data to which the reference refers changes before the thread is executed, those changes will be visible to the thread. We will see other examples of such behavior later in the course, as this is a primary source of concurrency bugs.


Intuition: The lambda which passes by value the id, gets id = 0. Also, the pass by reference function gets id=0, but, by the time the function finishes its work, id now has turned into 1.






\subsection{Starting a Thread with Variadic Templates and Member Functions}%
\label{sec:starting_a_thread_with_variadic_templates_and_member_functions}


\textbf{Variadic Templates}: They allow the definition of functions that take a varying number of arguments which is handy in concurrency because we can use them to pass several arguments to a thread. 

\textbf{Como nota importante:} con Variadic Templates se pasan los argumentos en el mismo orden.

\textbf{Member Functions} We can pass arguments to a thread by reference, but what if we wish to run a function other than the function call operator. We want to run functions of already existent objects. 



We have seen that one way to pass arguments in to the thread function is to package them in a class using the function call operator. Even though this worked well, it would be very cumbersome to write a special class every time we need to pass data to a thread. We can also use a Lambda that captures the arguments and then calls the function. But there is a simpler way: The thread constructor may be called with a function and all its arguments. That is possible because the thread constructor is a \textit{variadic template} that takes multiple arguments.


\subsubsection{Variadic Templates}%
\label{sub:variadic_templates}



The need for \textbf{variadic templates} could be illustrated in the Listing~\ref{lst:variadic_templates}. The second thread object is constructed with a function \texttt{printIDAndName}, which requires an int and a string, but if we only pass a single argument to the thread when calling\texttt{printIDAndName} a compiler error will occur.  



\begin{listing}
\begin{minted}[linenos,numbersep=5pt,frame=lines,framesep=2mm]{cpp}
#include <iostream>
#include <thread>
#include <string>

void printID(int id)
{
    std::this_thread::sleep_for(std::chrono::milliseconds(50));
    std::cout << "ID = " << id << std::endl;
    
}

void printIDAndName(int id, std::string name)
{
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
    std::cout << "ID = " << id << ", name = " << name << std::endl;
}

int main()
{
    int id = 0; // Define an integer variable

    // starting threads using variadic templates
    std::thread t1(printID, id);
    std::thread t2(printIDAndName, ++id, "MyString");
    std::thread t3(printIDAndName, ++id); // this procudes a compiler error

    // wait for threads before returning
    t1.join();
    t2.join();
    //t3.join();


    return 0;
}
\end{minted}
\caption{Variadic Templates}
\label{lst:variadic_templates}
\end{listing}




There is one more difference between calling a function directly and passing it to a thread: With the former, arguments may be passed by value, by reference or by using move semantics - depending on the signature of the function. When calling a function using a variadic template, the arguments are by default either moved or copied - depending on wether they are rvalues or lvalues. There are ways however which allow us to overwrite this behavior. If you want to move an lvalue for example, we can call std::move. 


In the file \texttt{MoveVsCopybyValue.cpp}, two threads are started. One where the string is \textbf{copied by value}. This allows us to print name1 even after \texttt{join} has been called. The second one is passed to the thread function using \textbf{move semantics}, which means that it is not available any more after \texttt{join} has been called.


In the file \texttt{PassDataAsRef.cpp}, when passing the string variable name to the thread funciton, we need to explicity mark it as a reference, so the compiler threat it as such. This is done using \texttt{std::ref}.


Even though the code works, we are now sharing mutable data between threads - \textbf{which is a primary source for concurrency bugs.}

\subsubsection{Starting Threads with Member Functions}%
\label{sub:starting_threads_with_member_functions}



What if we want to run a member function other than the function call operator, such as a member function of an existing object?. In C++, for calling member functions, the \texttt{std::thread} function requires an additional argument for the object on which to invoke the member function.


In the \texttt{ThreadsMemberFunc.cpp}, the Vehicle object v1 is passed to the thread function by value, thus a copy is made which does not affect the \textit{original} living in the main thread. That's why changes to its members variable \texttt{\_id} will thus not show when printing later in main. 

In the second Vehicle object, because it's passed by reference, changes to its \texttt{\_id} variable will also be visible in the main thread. 


Another thing to consider is that we have to ensure that the existence of \texttt{v2} outlives the completion of the thread \texttt{t2} - otherwise there will be an attempt to access an invalidated memory address. An alternative is to use a heap-allocated object and a reference-counted pointer such as \texttt{std::shared\_ptr<Vehicle>} to ensure that the object lives as long as it takes the thread to finish its work. The file 




\subsection{Running Multiple Threads}%
\label{sec:running_multiple_threads}


\subsubsection{Fork-Join Parallelism}%
\label{sub:fork_join_parallelism}

The basic mechanism of this concept follows a simple three-step pattern:

\begin{enumerate}
	\item Split the flow of execution into a parallel thread ( \textit{"fork"})
	\item Perform some work in both the main thread and the parallel thread.
	\item Wait for the parallel thread to finish and unite the split flow of execution again \textit{("join")}.
\end{enumerate}





\begin{figure}[htpb!]
	\centering
	\includegraphics[width=0.8\linewidth]{fork_join}
	\caption{Fork-Join Parallelism}
	\label{fig:fork_join_parallelism}
\end{figure}

In the main thread, the program flow is forked into three parallel branches. In both worker branches, some work is performed - which is why threads are often referred to as \textit{"worker threads"}. Once the work is completed, the flow of execution is united again in the main function using the \texttt{join()}  command. In this example, join acts as a barrier where all threads are united. The execution of main is in fact halted, until both worker threads have successfully completed their respective work. 



In the \texttt{forkJoinParallelism.cpp} file, if we try to compile the program using the \texttt{push\_back()} function, we get a compiler error. The problem with our code is that by pushing the thread object into the vector, we attempt to make a copy of it. However, thread objects do not have a copy constructor and thus can not be duplicated. If this were possible, we would create yet another branch in the flow of execution - which is not what we want. The solution to this problem is to use move semantics, which provide a convenient way for the contents of objects to be 'moved' between objects, rather than copied. It might be a good idea at this point to refresh your knowledge on move semantics, on rvalues and lvalues as well as on rvalue references, as we will make use of these concepts throughout the course.

To solve our problem, we can use the function \texttt{emplace\_back()} instead of \texttt{push\_back()} which internally uses move semantics to move our thread object into the vector without making a copy. 

The console output is really messy. the components are completely intermingled. Also, when the program is run several times, the output will look different with each execution.


\begin{itemize}
	\item The order in which threads are executed is non-deterministic. Every time a program is executed, there is a chance for a completely different order of execution.
	\item Threads may get preempted in the middle of execution and another thread may be selected to run.
\end{itemize}


\textbf{These two properties pose a major problem with concurrent applications: A program may run correctly for thousands of times and suddenly, due to a particular interleaving of threads, there might be a problem. From a debugging perspective, such errors are very hard to detect as they can not be reproduced easily.}




In the file \textbf{properMultipleThreads.cpp}: the bug comes from accessing a shared memory location. (Remember that the lambda function in the code used \texttt{id}  by reference.)







\section{Passing Data Between Treads}%
\label{sec:passing_data_between_treads}


\begin{enumerate}
	\item We are going to look at the private communication channel, between two threads that can be used only a single time but at an arbitrary moment in time of our choosing. The concept behind this is called \textbf{promises and futures} and this can be used to pass data as well as exceptions from a worker thread to the parent thread which created the worker.   
	\item We are going to look at the differences between \textbf{threads} and \textbf{tasks.} \textbf{Tasks} are a high level concept where the system can choose whether to execute some code in parallel or synchronously. Also, they make easy to establish a promise future link in our code. This is much less code to write than standard \textbf{threads} we have usen so far. 
	\item Look at the runtime advantage or disadvantage of parallel execution.
	\item Passing data to a thread by value (making a copy of it) as well as using \textbf{move semantics.} Move semantics which avoids making unnecessary copies of your data, while at the same time avoiding some of the dangers of passing by reference. (which has an impact of the variables at main). 
	\item Look at data races which are one of the primary sources of error on concurrent programming. 
\end{enumerate}


\subsection{Promises and Futures}%
\label{sub:promises_and_futures}

Previously, we have look in the ways we can pass data from the parent thread to the worker thread. Now we want to move data from the worker to the parent thread (in this case, the main function). The sending of the channel is \textbf{promise} and the receiving is called \textbf{future} 


We can create a thread that takes a function and we will pass it the promise as an argument as well as the message to be modified. \textbf{Promises} cannot be copied, because the \textbf{promise-future} concept is a two-point communication channel for one-time use. Therefore, we must pass the promise to the thread function using the \texttt{std::move}. 

The thread function takes the promise as an rvalue reference in accordance with move semantics \textbf{(? qué significa esto¿)}. After waiting for several seconds, the message is modified and the method \texttt{set\_value()} is called on the promise. 


Back in the main thread, after starting the thread, the original message is printed to the console. Then, we start listening on the other end of the communication channel by calling the function \texttt{get()} on the future. This method will block until data is available - which happens as soon as \texttt{set\_value} has been called on the promise (from the thread). If the result is movable (which is the case for \texttt{ std::string}), it will be moved - otherwise it will be copied instead. After the data has been received (with a considerable delay), the modified message is printed to the console.


It's also possible that the worker value calls \texttt{set\_vale()} on the promise before \texttt{get()} is called on the future. In this case, \texttt{get()} return immediately without any delay. After \texttt{get()} has been called once, the future is no longer usable. This makes sense as the normal mode of data exchange between \textbf{promise} and \textbf{future} works with \texttt{std::move} - and in this case, the data is no longer available in the channel after the first call to \texttt{get()}. If \texttt{get()} is called a second time, an exception is thrown.



\subsubsection{Get vs Wait}%
\label{ssub:get_vs_wait}


There are some situations where it might be interesting to separate the waiting for the content from the actual retrieving. Futures allow us to do that using the \texttt{wait()} function. This method will block until the future is ready. Once it returns, it is guaranteed that data is available and we can use \texttt{get()} to retrieve it without delay.



In addition to wait, the C++ standard also offers the method \texttt{wait\_for}, which takes a time duration as an input and also waits for a result to become available. The method \texttt{wait\_for()} will block either until the specified timeout duration has elapsed or the result becomes available - whichever comes first. The return value identifies the state of the result.



\subsubsection{Passing Exceptions}%
\label{ssub:passing_exceptions}


The future-promise communication channel may also be used for passing exceptions. This is in the file \texttt{passingExceptions.cpp}.

In the thread function, we need to implement a try-catch block which can be set to catch a particular exception or - as in our case - to catch all exceptions. Instead of setting a value, we now want to throw a \texttt{std::exception} along with a customized error message. In the catch-block, we catch this exception and throw it to the parent thread using the promise with \texttt{set\_exception}. The function \texttt{std::current\_exception} allows us to easily retrieve the exception which has been thrown.

On the parent side, we now need to catch this exception. In order to do this, we can use a try-block around the call to get(). We can set the catch-block to catch all exceptions or - as in this example - we could also catch a particular one such as the standard exception. Calling the method what() on the exception allows us to retrieve the message from the exception - which is the one defined on the promise side of the communication channel.

\textbf{When we run the program, we can see that the exception is being thrown in the worker thread with the main thread printing the corresponding error message to the console.}

So a promise future pair can be used to pass either values or exceptions between threads.


One of the nice thing of using futures is that you can determine the point in time when you want to retrieve data by calling the \texttt{get()}  function. That can be very useful especially when you want to ensure that the result is only available at a certain point in time. Now, there are two major drawbacks from promises and futures.

\begin{enumerate}
	\item Information can only be passed into one direction. (from worker to parent).
	\item They are made for one-time use only. 
\end{enumerate}



\subsection{Threads vs Tasks}%
\label{sub:threads_vs_tasks}


If you want to quickly return data or an exception from worker to parent, a much simpler way than doing it with futures and promises is using \textbf{standard async} instead of standard thread. Calling \textbf{async} generates something called a \textbf{task} which is a high level construct where the system can decide whether to run it in parallel or  asynchronously.  



In the file \texttt{threadsWithAsync.cpp} we are going to change to code so we use \texttt{st::async}  instead of \texttt{std::thread}. 

The first change we are making is in the thread function: We are removing the promise from the argument list as well as the try-catch block. Also, the return type of the function is changed from void to double as the result of the computation will be channeled back to the main thread using a simple return. After these changes, the function has no knowledge of threads, nor of futures or promises - it is a simple function that takes two doubles as arguments and returns a double as a result. Also, it will throw an exception when a division by zero is attempted.

In the main thread, we replace \texttt{std::thread} with \texttt{std::async}. Note that async returns a future, which we will use later in the code to retrieve the value that is returned by the function. A promise, as with \texttt{std::thread}, is no longer needed, so the code becomes much shorter. In the try-catch block, nothing has changed - we are still calling \texttt{get()} on the future in the try-block and exception-handling happens unaltered in the catch-block. Also, we do not need to call \texttt{join()} any more. With async, the thread destructor will be called automatically - which reduces the risk of a concurrency bug.

When we run with \texttt{async}, as expected, the ids between the two threads (main and function) differ from each other - they are running in parallel. However, one of the major differences between \texttt{std::thread} and \texttt{std::async} is that with the latter, the system decides whether the associate function should be run asynchronously or synchronously. We can directly influence on this by adjusting the launch parameters of \texttt{std::async}. 

This is reflected in the line \texttt{std::future<double> ftr = std::async(std::launch::deferred, ...)} of the file \texttt{asyncForceParallel.cpp }


If we were to use the launch option \textit{"async"} instead of \textit{"deferred"} we would enforce an asynchronous execution whereas the option \textit{"any"} would leave it to the system to decide (which is the default).

At this point, let us compare std::thread with std::async: Internally, std::async creates a promise, gets a future from it and runs a template function that takes the promise, calls our function and then either sets the value or the exception of that promise - depending on function behavior. The code used internally by std::async is more or less identical to the code we used in the previous example (\texttt{passinExceptions.cpp} in the PromisesAndFutures folder)  , except that this time it has been generated by the compiler and it is hidden from us - which means that the code we write appears much cleaner and leaner. Also, std::async makes it possible to control the amount of concurrency by passing an optional launch parameter, which enforces either synchronous or asynchronous behavior. This ability, especially when left to the system, allows us to prevent an overload of threads, which would eventually slow down the system as threads consume resources for both management and communication. If we were to use too many threads, the increased resource consumption would outweigh the advantages of parallelism and slow down the program. By leaving the decision to the system, we can ensure that the number of threads is chosen in a carefully balanced way that optimizes runtime performance by looking at the current workload of the system and the multi-core architecture of the system.



\subsubsection{Task-based Concurrency}%
\label{ssub:task_based_concurrency}

Determining the optimal number of threads to use is a hard problem. It usually depends on the number of available cores whether it makes sense to execute code as a thread or in a sequential manner. The use of \texttt{std::async} (and thus tasks) take the burden of this decision away from the user and let the system decide whether to execute the code sequentially or as a thread. With tasks, the programmer decides what \textbf{CAN} be run in parallel in principle and the system then decides at runtime what \textbf{WILL} be run in parallel.

Internally, this is achieved by using thread-pools which represent the number of available threads based on the cores/processors as well as by using work-stealing queues, where tasks are re-distributed among the available processors dynamically.


We can see the idea of \textbf{work-stealing queue} in the Figure~\ref{fig:work_stealing_queues}. The first core in the example is heavily oversubscribed with several tasks that are waiting to be executed. he idea of a work-stealing queue is to have a watchdog program running in the background that regularly monitors the amount of work performed by each processor and redistributes it as needed. So, in our example, the tasks waiting would be shifted (or \textit{"stolen"}) from busy cores and added to available free cores.

A work distribution nin this manner can only work, when parallelism is explicitly described in the program by the programmer. If this is not the case, work-stealing will not perform effectively.

\begin{figure}[htpb!]
\begin{subfigure}{.5\textwidth}
  \centering
  % include first image
  \includegraphics[width=.9\linewidth]{task_dist_1}  
  \caption{Before}
	\label{fig:before}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  % include second image
  \includegraphics[width=.9\linewidth]{task_dist_2}  
  \caption{After}
	\label{fig:after}
\end{subfigure}
\caption{Work Stealing Queues}
\label{fig:work_stealing_queues}
\end{figure}





\textbf{Task-based vs Thread-based} programming:

\begin{table}[htpb!]
	\centering
	\caption{Tasks vs Threads}
	\label{tab:tasks_vs_threads}
	\begin{tabular}{|c|c|}
		\hline
	  \textbf{Task-based} & \textbf{Thread-based} \\
		\hline
		\hline
	  System takes care of many detail  & Programmer is responsible \\
		(e.g. join) &  for many details \\
		\hline
		Light-weight  & Heavy-weight (they are generated by the OS) \\
		because they will be using a & It takes time for the OS \\
		pool of already created & to allocate memory / stack /  \\
		threads  & kernel data for the thread \\
		\hline
		Cheaper to destroy    & Expensive to destroy \\
		\hline
		Focused on throughput & Focused on latency \\
	  \hline
		Concurrency abstraction is high & Concurrency abstraction is low \\
		\hline

	\end{tabular}
\end{table}



Threads have more to do with latency. When you have functions that can block (e.g. file input, server connection), threads can avoid the program to be blocked, when e.g. the server is waiting for a response. Tasks on the other hand focus on throughput, where many operations are executed in parallel.


In the file \texttt{performanceOfThreads.cpp} the main idea is to see if executing the threads in parallel makes sense. The parameters that we can change are \texttt{nLoops, nThreads, std::launch::async or deferred}. The result is that is we have a small number of loops, it doesn't make sense to run them in parallel. 



\subsubsection{Outro}%
\label{ssub:outro}

So we have two basic possibilities to start threads. We can either use \texttt{std::thread}  or \texttt{std::async}. As a general rule of thumb, using async as in many cases is the easiest solution which involves less boilerplate code and basically you can achieve the same as with \texttt{std::thread}. With async you don't have to worry about calling \texttt{join()}  in the parent thread and also there's no \textbf{promise} you need to carry around and manage. 

We might consider using threads when we want to have more control. Don't think to use async to launch tasks that do input-output manipulate mutexes or in other ways, interact with other threads. The idea is to use async in the simples and more common cases. 



\subsection{Avoiding Data Races}%
\label{sub:avoiding_data_races}

\textbf{Data Races} occur, when two concurrent threads are accessing the same memory location while at least one of them is modifying (the other thread might be reading or modifying). In this scenario, the value at the memory location is completely undefined. Depending on the system scheduler, the second thread will be executed at an unknown point in time and thus see different data at the memory location with each execution. Depending on the type of program, the result might be anything from a crash to a security breach when data is read by a thread that was not meant to be read, such as a user password or other sensitive information. Such an error is called a data race because two threads are racing to get access to a memory location first, with the content at the memory location depending on the result of the race.




We'll discuss some strategies for avoid them:

\begin{enumerate}
	\item Overwriting the copy constructor of a class to ensure a proper deep copy of all data structures as performed.  
	\item Move semantics 
	\item carefully synchronize the two threads using the \texttt{join()} or the promise-future concept that can guarantee the availability of a result.
\end{enumerate}



In the file \texttt{DataRaceExample\_atomic.cpp}, an instance of the proprietary class \texttt{Vehicle} is created and passed to a thread by value, thus making a copy of it.

Note that the class Vehicle has a default constructor and an initializing constructor. In the main function, when the instances v0 and v1 are created, each constructor is called respectively. Note that v0 is passed by value to a Lambda, which serves as the thread function for std::async. Within the Lambda, the id of the Vehicle object is changed from the default (which is 0) to a new value 2. Note that the thread execution is paused for 500 milliseconds to guarantee that the change is performed well after the main thread has proceeded with its execution.

In the main thread, immediately after starting up the worker thread, the id of v0 is changed to 3. Then, after waiting for the completion of the thread, the vehicle id is printed to the console.

Passing data to a thread in this way is a clean and safe method as there is no danger of a data race - at least when atomic data types such as integers, doubles, chars or booleans are passed.


Now, in the file \texttt{DataRaceExample\_pointer.cpp} we show the fact that when passing a complex data structure however, there are sometimes pointer variables hidden within, that point to a potentially shared data buffer - which might cause a data race. 


Even though v0 is being copied by value when passed to the thread function, this time, however, even though a copy has been made, the original object \texttt{v0} is modified, when the thread function sets the new name. This happens because the member \texttt{\_name} is a pointer to a string and after copying, even though the pointer variable has been duplicated, it still points to the same location as its value (i.e. the memory location) has not changed. Note that when the delay is removed in the thread function, the console output varies between "Vehicle 2" and "Vehicle 3", depending on the system scheduler. Such an error might go unnoticed for a long time. It could show itself well after a program has been shipped to the client



Classes from the standard template library usually implement a deep copy behavior by default (such as std::vector). When dealing with proprietary data types, this is not guaranteed. The only safe way to tell whether a data structure can be safely passed is by looking at its implementation: Does it contain only atomic data types or are there pointers somewhere? If this is the case, does the data structure implement the copy constructor (and the assignment operator) correctly? Also, if the data structure under scrutiny contains sub-objects, their respective implementation has to be analyzed as well to ensure that deep copies are made everywhere.



\textbf{Overwriting Copy Constructor}

The problem with passing a proprietary class is that the standard copy constructor makes a 1:1 copy of all data members, \textbf{including pointers to objects}. We would like a \textit{"Deep Copy"}, that is, copying the data to which the pointer refers. 


In the file \texttt{OverwriteCopyConstructor.cpp} we make a deep copy so that the program always prints "Vehicle 3" to the console, regardless of the delay within the thread function.





\subsubsection{Passing data using move semantics}%
\label{ssub:passing_data_using_move_semantics}

Even though a customized copy constructor can help us to avoid data races, it is also time (and memory) consuming. In the following, we will use move semantics to implement a more effective way of safely passing data to a thread.

A move constructor enables the resources owned by an rvalue object to be moved into an lvalue without physically copying it. Rvalue references support the implementation of move semantics, which enables the programmer to write code that transfers resources (such as dynamically allocated memory) from one object to another.

Unlike the default copy constructor however, the compiler does not provide a default move constructor. 


\subsubsection{Move Semantics and Uniqueness}%
\label{ssub:move_semantics_and_uniqueness}

As with the above-mentioned copy constructor, passing by value is usually safe - provided that a deep copy is made of all the data structures within the object that is to be passed. With move semantics , we can additionally use the notion of uniqueness to prevent data races by default. In the file \texttt{moveSemanticsAndUniqueness.cpp}  example, a \texttt{unique\_pointer} instead of a raw pointer is used for the string member in the Vehicle class.


\textbf{In this file, the move constructor transfers the unique pointer to the worker by using \texttt{std::move}  and thus invalidates the pointer in the main thread.} When calling \texttt{v0.getName()}, an exception is thrown, making it clear to the programmer that accessing the data at this point is not permissible - which is the whole point of using a unique pointer here as a data race will now be effectively prevented. 

The point of this example has been to illustrate that move semantics on its own is not enough to avoid data races. The key to thread safety is to use move semantics in conjunction with uniqueness. It's the responsibility of the programmer to ensure that pointers to objects that are moved between threads are unique.









\end{document}
